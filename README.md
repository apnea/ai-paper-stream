# ai-paper-stream
a stream of interesting papers read or to be read

---
1. **[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764):** LLM is ternary {-1, 0, 1}, BitNet b 1.58. Resource use and performance benefits. Could open up new avenues for arch design
   
2. **[Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/pdf/2311.11045.pdf):** still fun to plug the questions into newer models. E.g. Mistral large, which just cruises through them.

3. **[Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf):** THE original Triton paper. So modest. But all your base is belong to us. One script to rule them all, and in the tensor to bind them. https://github.com/openai/triton https://triton-lang.org/main/index.html
