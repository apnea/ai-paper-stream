# ai-paper-stream
a stream of interesting papers read or to be read

---
1. **[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764):** LLM is ternary {-1, 0, 1}, BitNet b 1.58. Resource use and performance benefits. Could open up new avenues for arch design
   
2. **[Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/pdf/2311.11045.pdf):** still fun to plug the question into newer models. E.g. Mistral large, which just cruises through them
